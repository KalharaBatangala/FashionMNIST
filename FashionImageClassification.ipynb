{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSlgeerkQ62AnvtDuUaHpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KalharaBatangala/FashionMNIST/blob/main/FashionImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4W4elxSA-SE-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from PIL import Image   # just import only class Image\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening destroys locality\n",
        "<br>Convolutions preserve it"
      ],
      "metadata": {
        "id": "HCMh-HiHNWOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PIL images to pytorch tensors\n",
        "transforms = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "_VtwTbvrN0UZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load train dataset\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "# no of training exmples\n",
        "print(f'Number of training examples {len(train_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtMUqjqKOHW_",
        "outputId": "832b297d-dd54-469e-c260-68979ed52ecc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 305kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.62MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 25.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples 60000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]  # take one training exmple\n",
        "\n",
        "print(image.shape)  # [1,28,28] (C, H, W) because it is a pytorch tensor\n",
        "img_2d = image.squeeze()  # remove channel dimension\n",
        "print(img_2d.shape) # now (H, W)\n",
        "plt.imshow(img_2d, cmap='gray')  # it work with matplotlib\n",
        "plt.title(f\"Label : {label}\")\n",
        "plt.axis(\"off\")\n",
        "# matplotlib expects ---> (H, W) for grayscale and (H, W, C) for color\n",
        "# pytorch returns ---> (C, H, W) but this is different in tensorflow. Channel last"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "3MQ6YJh2R1cf",
        "outputId": "b0cff6e7-56de-4c29-b6c2-d6eb2d062ce0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in pytorch, channel is the first value <br>\n",
        "torch.Size([1,28,28]) only 1 channel"
      ],
      "metadata": {
        "id": "lIw20BEzSzX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load test dataset\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms\n",
        ")\n"
      ],
      "metadata": {
        "id": "tQt3lY1x8tOL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of test examples {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkdfy09a9Wza",
        "outputId": "d65488f3-011f-4ad3-8836-66f1c4b2dec2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test examples 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "7qMqeTDs94GQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "40IwVTaM9-sP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one batch\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "print(\"Batch image shape:\", images.shape)\n",
        "print(\"Batch labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52QeeRbZ_Hnf",
        "outputId": "2d4bfe2b-4f18-4611-cc34-06f4ba815017"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch image shape: torch.Size([64, 1, 28, 28])\n",
            "Batch labels shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "jqNX3gfyBzNd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional block - 1**"
      ],
      "metadata": {
        "id": "H_6X1dMA6luO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer = nn.Conv2d(\n",
        "    in_channels=1,\n",
        "    out_channels=32,\n",
        "    kernel_size=3,\n",
        "    stride=1,\n",
        "    padding=1   # keep spatial size\n",
        ")\n",
        "\n",
        "relu = nn.ReLU()\n",
        "pool = nn.MaxPool2d(\n",
        "    kernel_size=2,    # half the size\n",
        "    stride=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "CpWbmua8CNgr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a feature extractor. Next we want a classifier, <br>\n",
        "that means a fully connected layer"
      ],
      "metadata": {
        "id": "r4ajr7GV7ChV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass one batch through the block\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "x = conv_layer(images)\n",
        "x = relu(x)\n",
        "x = pool(x)\n",
        "\n",
        "print(\"After Conv:\", conv_layer(images).shape)\n",
        "print(\"After ReLU:\", relu(conv_layer(images)).shape)\n",
        "print(\"After Pool:\", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ0tnIonCusK",
        "outputId": "c8aac27b-7f2b-4ce6-da75-8ffe88a8b6d3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Conv: torch.Size([64, 32, 28, 28])\n",
            "After ReLU: torch.Size([64, 32, 28, 28])\n",
            "After Pool: torch.Size([64, 32, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.Size([64, 32, 28, 28]) <br>\n",
        "this always follows the pytorch's convention (N, C, H, W)\n",
        "<br>\n",
        "64 = batch size (N) meaning 64 different stacks of\n",
        "<br> 32 feature maps"
      ],
      "metadata": {
        "id": "N3aE7a_YLI0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNNs do NOT detect objects <br> <br>\n",
        "They detect **features**"
      ],
      "metadata": {
        "id": "UyrnO7SJDKQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classifier**"
      ],
      "metadata": {
        "id": "Ln99pLZe8iwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fc = nn.Linear(32 * 14 * 14, 10)  # 10 classes in fashionMNIST\n",
        "\n",
        "# Forward pass continued\n",
        "x = x.view(x.size(0), -1)  # Flatten\n",
        "\n",
        "print(\"After flatten:\", x.shape)\n",
        "\n",
        "x = fc(x)\n",
        "\n",
        "print(\"After Linear:\", x.shape)   # gives 10 output logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfmUEExmDSHE",
        "outputId": "5a73f5bf-c90c-4f2e-fef2-aa1221f40e9b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After flatten: torch.Size([64, 6272])\n",
            "After Linear: torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logits = raw, unnormalized scores produced by the model"
      ],
      "metadata": {
        "id": "OnDm7IASTGqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.Size([64, 6272])\n",
        "\n",
        "# column vector of 6272"
      ],
      "metadata": {
        "id": "cGMSxzuCSmyn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOSS + BACKPROPAGATION**"
      ],
      "metadata": {
        "id": "7Hc6ol3VYPIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss function\n",
        "criterion = nn.CrossEntropyLoss()   # expects logits\n",
        "\n",
        "# Compute Loss\n",
        "loss = criterion(x, labels)\n",
        "\n",
        "print(\"Loss value: \", loss.item())\n",
        "\n",
        "# backward pass\n",
        "loss.backward()     # compute gradients but do not update weights\n",
        "\n",
        "print(\"Backward pass completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CHAGFGzYOgb",
        "outputId": "b0549ec1-fba0-4ce6-cf58-f3c8191dc255"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss value:  2.2900590896606445\n",
            "Backward pass completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "average loss per image in the batch: **2.3259010314941406**"
      ],
      "metadata": {
        "id": "a9Og0SVzjG8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(fc.weight.grad.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkLXXNpmZzGa",
        "outputId": "c3e32628-c169-4f2e-8803-96baf997ac85"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 6272])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer**"
      ],
      "metadata": {
        "id": "Cho9El_DnjUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer\n",
        "optimizer = optim.SGD(\n",
        "    fc.parameters(),\n",
        "    lr=0.01\n",
        ")\n",
        "\n",
        "\n",
        "# clear gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# update\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Weights updated for one step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqGLh4SQnkyO",
        "outputId": "4aa0aecf-c390-4e57-f37a-844e48b0b813"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights updated for one step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why SGD?**  <br>\n",
        "simple and predictable <br>\n",
        "θ = θ - lr * gradient\n"
      ],
      "metadata": {
        "id": "5ElM1yZPpneE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term                  | Meaning                                                                                           | Typical Usage                            |\n",
        "| --------------------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------- |\n",
        "| **Transfer Learning** | Take a **pretrained model** on large dataset (like ImageNet) and use it for your task             | Freeze conv layers → train only new head |\n",
        "| **Fine-Tuning**       | Take **pretrained model**, optionally unfreeze some or all layers, and train them on your dataset | Gradually update conv layers + new head  |\n"
      ],
      "metadata": {
        "id": "Nbs3Xh10uBRS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdksNhIYuQvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "i071Uyf6unqj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, in_channels=1, num_classes=10):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    # conv layer 1\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # conv layer 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "    # fc layer\n",
        "    self.fc = nn.Linear(64 * 7 * 7, num_classes)  # 64 is the out channels after 2nd convo layer, not batch size\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # passing through 2nd convo block\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = x.view(x.size(0), -1) # flatten before fc layer\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "FJ4rBnLduo5y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# define loss\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer/ adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# clear before update\n",
        "optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "q4my7OJk0w73"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch of data\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# forward pass\n",
        "outputs = model(images)\n",
        "\n",
        "# compute loss\n",
        "loss = criterion(outputs, labels)\n",
        "\n",
        "print(\"Loss: \", loss.item())  # since criterion defined with reduction = 'mean'\n",
        "\n",
        "# compute gradients\n",
        "loss.backward()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoLa5sHN2KGl",
        "outputId": "bb23c894-dcdc-4855-ee3b-7871fae5d78a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  2.3157966136932373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Weights updated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NclDzr5V2xfg",
        "outputId": "7376db3c-7aa9-4844-bfb9-a5ecbd9ac7f9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights updated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "3GThpqtn3AvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    # clear grads\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    outputs = model(images)\n",
        "\n",
        "    # compute loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # backprop\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()\n",
        "\n",
        "    # accumulate loss for reporting\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  # print avg loss for epoch\n",
        "  print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XZe3bUe3BxL",
        "outputId": "2ca4223f-e854-4dbc-b87b-01fa8191a379"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.3113\n",
            "Epoch [2/5], Loss: 0.2705\n",
            "Epoch [3/5], Loss: 0.2427\n",
            "Epoch [4/5], Loss: 0.2238\n",
            "Epoch [5/5], Loss: 0.2071\n",
            "Training completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute training accuracy per epoch\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Training Accuracy: {100*correct/total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA9ZmGkc7c67",
        "outputId": "b34f3aa0-7234-43b7-b541-70b6663b1297"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 93.20%\n"
          ]
        }
      ]
    }
  ]
}